{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Basics of Natural Language Processing (NLP)Take Home Exercise #\n",
        "\n"
      ],
      "metadata": {
        "id": "8AhFOUj6iCWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the following link to find open source data sets to complete take-home exercises.\n",
        "\n",
        "[Data Sets](https://opendatascience.com/20-open-datasets-for-natural-language-processing/)"
      ],
      "metadata": {
        "id": "zwEukFd8AJE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run this code in the beginning to limit the output size of the cells"
      ],
      "metadata": {
        "id": "fxBFGZ8DadFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " from IPython.display import display, Javascript\n",
        "\n",
        "def resize_colab_cell():\n",
        "  # Change the maxHeight variable to change the max height of the output\n",
        "   display(Javascript('google.colab.output.setIframeHeight(0, true, {maxHeight: 400})'))\n",
        "  #Change output size for the entire notebook (set to call function on cell run)\n",
        "   get_ipython().events.register('pre_run_cell', resize_colab_cell)"
      ],
      "metadata": {
        "id": "8HBE7ck1XPX_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Input Text\n",
        "\n",
        "Write a code to collect text for the analysis as a user input\n",
        "\n"
      ],
      "metadata": {
        "id": "jT3X0RuyndZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write a code to collect text for the analysis as a user input\n",
        "\n",
        "text = input(\"Enter the text for analysis: \")\n",
        "print(\"Entered text:\", text)\n"
      ],
      "metadata": {
        "id": "ER80G_dwRkij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "f9c1c082-8449-4ce8-ebb5-8875826e275b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-91d12f136d95>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# prompt: Write a code to collect text for the analysis as a user input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the text for analysis: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Entered text:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Basic Analysis\n",
        "\n",
        "Perform basic text analysis on the collected text using Spacy ([spacy.io](http://spacy.io)) library. Try different string manipulations."
      ],
      "metadata": {
        "id": "UI_3GXkzn5U9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import spacy library\n",
        "import spacy\n",
        "\n",
        "# Load the small English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Get the user input (from the existing input function in the notebook)\n",
        "text = input(\"Enter the text for analysis: \")\n",
        "\n",
        "# Process the text using SpaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print basic information: tokenization, POS tagging, dependency parsing, and named entities\n",
        "print(\"Tokenization and POS Tagging:\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text:10}  {token.pos_:10}  {token.dep_:10}  {token.lemma_:10}\")\n",
        "\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text:10}  {ent.label_:10}  {spacy.explain(ent.label_)}\")\n",
        "\n",
        "# Display sentence boundaries\n",
        "print(\"\\nSentence Boundaries:\")\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)\n"
      ],
      "metadata": {
        "id": "n8WSsfmTj8en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Tokenizer\n",
        "Create a custom tokenizer in Python that handles:\n",
        "*   Contractions (e.g., \"don't\" → \"do n't\")\n",
        "*   Keeps punctuation as separate tokens\n",
        "*   Splits hyphenated words (e.g., \"state-of-the-art\" → \"state of the art\")\n",
        "\n",
        "Compare its results with NLTK's word_tokenize on any sample paragraph and the following examples:\n",
        "\"New York-based company\", \"It's a beautiful day!\", \"https://www.example.com\"\n",
        "\n",
        "What differences do you see? What are the advantages, and limitations of each approach?"
      ],
      "metadata": {
        "id": "fER49EQBRZlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Regex\n",
        "\n",
        "Try writing your own RegEx that can capture citations in text E.g. (Horning, 2022)"
      ],
      "metadata": {
        "id": "iQSBeMtujknV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\\([A-Za-z\\-\\']+,\\s\\d{4}\\)"
      ],
      "metadata": {
        "id": "iI5xT7o8Rlcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract URLS following a certain format (www. or http or https:// ..)"
      ],
      "metadata": {
        "id": "DCuiR4kZCp32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(https?:\\/\\/|www\\.)[^\\s]+"
      ],
      "metadata": {
        "id": "V49uczwjCyd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Word Frequency\n",
        "\n",
        "Find the list of words that occur more than 10 times in a selected corpus.\n",
        "\n",
        "Try using different forms of setup: no stopwords, custom stopwords, not removing punctuation, etc. and see what difference in results they produce.\n"
      ],
      "metadata": {
        "id": "tBIAVwtV_cvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Example corpus\n",
        "corpus = \"\"\"This is a sample corpus. This corpus contains text that repeats words,\n",
        "such as 'this', 'corpus', and 'words'. Let's see how many times words appear.\"\"\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = re.findall(r'\\w+', corpus.lower())\n",
        "\n",
        "# Count word frequency\n",
        "word_freq = Counter(words)\n",
        "\n",
        "# Filter words that occur more than 10 times\n",
        "frequent_words = {word: count for word, count in word_freq.items() if count > 10}\n",
        "\n",
        "print(frequent_words)\n"
      ],
      "metadata": {
        "id": "tNK4qAHUFUxT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}