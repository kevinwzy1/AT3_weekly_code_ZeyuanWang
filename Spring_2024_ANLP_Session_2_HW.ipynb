{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Use the following lists to find open source data sets to complete take-home exercises. You can also apply these in the data set provided for AT1.\n",
        "\n",
        "[Open Data Sets](https://canvas.uts.edu.au/courses/32341/pages/open-data-sets-for-nlp-and-text-analysis?module_item_id=1878922)"
      ],
      "metadata": {
        "id": "zwEukFd8AJE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing\n",
        "\n",
        "1.   Calculate word associations in a large data set; try different methods to calculate it (e.g. pmi, chi-square test, etc.)\n",
        "2.  Compare lemmatization and stemming results\n",
        "3.   Try adding another pre-processing step to remove all numbers/ digits from text\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jT3X0RuyndZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = \"\"\"This is a large dataset containing different word associations.\n",
        "            For example, word associations such as 'text analysis' or 'natural language'\n",
        "            are commonly found in this dataset.\"\"\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = nltk.word_tokenize(corpus.lower())\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words and re.match(r'\\w+', word)]\n",
        "\n",
        "# Use NLTK's BigramCollocationFinder to find word pairs\n",
        "finder = BigramCollocationFinder.from_words(filtered_words)\n",
        "\n",
        "# PMI calculation\n",
        "pmi_scores = finder.score_ngrams(BigramAssocMeasures.pmi)\n",
        "\n",
        "# Print top 5 word pairs with highest PMI\n",
        "print(\"Top 5 word pairs by PMI:\", pmi_scores[:5])\n"
      ],
      "metadata": {
        "id": "ER80G_dwRkij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Topic Modeling\n",
        "\n",
        "Try out Topic Modeling using the Sci-kit learn (SKLearn) package. There are different algorithms you can read about and experiment with - Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA), Non-negative Matrix Factorization (NMF)"
      ],
      "metadata": {
        "id": "iQSBeMtujknV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF\n",
        "\n",
        "# Example corpus\n",
        "corpus = [\n",
        "    \"Natural language processing is an interesting field.\",\n",
        "    \"Machine learning and deep learning are crucial for NLP tasks.\",\n",
        "    \"Text analysis involves extracting meaningful insights from text data.\",\n",
        "    \"Deep learning enables language models to learn context.\",\n",
        "    \"Latent Dirichlet Allocation is a popular topic modeling technique.\",\n",
        "    \"Non-negative Matrix Factorization helps in topic discovery.\",\n",
        "    \"Latent Semantic Analysis uncovers hidden structures in text data.\"\n",
        "]\n",
        "\n",
        "# Vectorize the corpus using TF-IDF for LSA and NMF, and Count for LDA\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "X_count = count_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Helper function to display topics\n",
        "def display_topics(model, feature_names, num_top_words):\n",
        "    for idx, topic in enumerate(model.components_):\n",
        "        print(f\"Topic {idx + 1}: \" + \" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
        "\n",
        "# 1. Latent Semantic Analysis (LSA)\n",
        "print(\"Latent Semantic Analysis (LSA) Topics:\")\n",
        "lsa = TruncatedSVD(n_components=2, random_state=42)\n",
        "lsa.fit(X_tfidf)\n",
        "display_topics(lsa, tfidf_vectorizer.get_feature_names_out(), 3)\n",
        "\n",
        "# 2. Latent Dirichlet Allocation (LDA)\n",
        "print(\"\\nLatent Dirichlet Allocation (LDA) Topics:\")\n",
        "lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
        "lda.fit(X_count)\n",
        "display_topics(lda, count_vectorizer.get_feature_names_out(), 3)\n",
        "\n",
        "# 3. Non-negative Matrix Factorization (NMF)\n",
        "print(\"\\nNon-negative Matrix Factorization (NMF) Topics:\")\n",
        "nmf = NMF(n_components=2, random_state=42)\n",
        "nmf.fit(X_tfidf)\n",
        "display_topics(nmf, tfidf_vectorizer.get_feature_names_out(), 3)\n"
      ],
      "metadata": {
        "id": "iI5xT7o8Rlcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Clustering\n",
        "\n",
        "Try out text clustering with a different dataset and build an optimized model by re-evaluating the number of clusters."
      ],
      "metadata": {
        "id": "tBIAVwtV_cvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample dataset of text documents\n",
        "corpus = [\n",
        "    \"Natural language processing is an exciting field of study.\",\n",
        "    \"Deep learning is essential for tasks in NLP.\",\n",
        "    \"Text analysis helps to derive insights from data.\",\n",
        "    \"Machine learning enables the creation of predictive models.\",\n",
        "    \"Clustering techniques like K-means help to organize data.\",\n",
        "    \"Topic modeling discovers hidden structures in data.\",\n",
        "    \"Supervised learning involves labeled data for model training.\"\n",
        "]\n",
        "\n",
        "# Step 1: Text Vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Step 2: Determine Optimal Number of Clusters using the Elbow Method\n",
        "inertia = []\n",
        "silhouette_scores = []\n",
        "k_values = range(2, 8)  # Testing different cluster numbers\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "    silhouette_avg = silhouette_score(X, kmeans.labels_)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "# Plot the Elbow Method\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(k_values, inertia, 'bo-')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method')\n",
        "\n",
        "# Plot Silhouette Score\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(k_values, silhouette_scores, 'bo-')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Analysis')\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Fit Optimal Model\n",
        "# Choose k based on the plots (e.g., k=3 if the elbow or highest silhouette score is at 3)\n",
        "optimal_k = 3\n",
        "kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "kmeans_optimal.fit(X)\n",
        "\n",
        "# Display clustering results\n",
        "for i, label in enumerate(kmeans_optimal.labels_):\n",
        "    print(f\"Document {i+1} is in Cluster {label}\")\n"
      ],
      "metadata": {
        "id": "n5f6rgjHXWaa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}